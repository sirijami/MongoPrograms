import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;


public class DistinctPatternDriver {

	public static void main(String[] args) {
		// TODO Auto-generated method stub

	}
	
	
	public class DistinctPatternMapper extends Mapper<Object, Text, Text, NullWritable>{
		Text ipAddress = new Text();

		@Override
		protected void map(Object key, Text value,
				Mapper<Object, Text, Text, NullWritable>.Context context)
				throws IOException, InterruptedException {
			
			StringTokenizer tokens = new StringTokenizer(value.toString());
			//System.out.println("tokens " + tokens);
			while(tokens.hasMoreTokens()){
				ipAddress.set(tokens.nextToken());
				context.write(ipAddress, NullWritable.get());
			}
		}		
	}
	
	public class DistinctPatterReducer extends Reducer<Text, NullWritable, Text, NullWritable>{

		@Override
		protected void reduce(Text arg0, Iterable<NullWritable> arg1,
				Reducer<Text, NullWritable, Text, NullWritable>.Context arg2)
				throws IOException, InterruptedException {
			
			arg2.write(arg0, NullWritable.get());
		}		
	}

}
